{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuqmb8GJFnXD",
        "outputId": "b8b85cab-fa4a-4f11-de60-03d505895f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: codebleu in /usr/local/lib/python3.12/dist-packages (0.7.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: tree-sitter<0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from codebleu) (0.22.3)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.12/dist-packages (from codebleu) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.121.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.4)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate codebleu datasets transformers peft accelerate torch scikit-learn gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from codebleu import calc_codebleu"
      ],
      "metadata": {
        "id": "iJkxSwIwoMb9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsv_path = '/content/spoc-train.tsv'\n",
        "df = pd.read_csv(tsv_path, sep='\\t', dtype=str).fillna('')\n",
        "\n",
        "def reconstruct(g):\n",
        "    pseudo = '\\n'.join(g['text'].astype(str).tolist()).strip()\n",
        "    code = '\\n'.join(g['code'].astype(str).tolist()).strip()\n",
        "    return pd.Series({'pseudo': pseudo, 'code': code})\n",
        "\n",
        "pairs = df.groupby(['probid','subid'], group_keys=False).apply(reconstruct).reset_index()\n",
        "print(f\"Total pairs: {len(pairs)}\")\n",
        "print(pairs.head(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55dZjWQdoQiZ",
        "outputId": "d400e1ae-1b2a-4eec-e1e9-bf71fadc2574"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pairs: 14548\n",
            "  probid     subid                                             pseudo  \\\n",
            "0  1000A  41887560  create a map from strings to integers mp\\n\\ncr...   \n",
            "1  1000A  41980279  INF = const int with INF = 0x3f3f3f3f\\n\\ni, j,...   \n",
            "\n",
            "                                                code  \n",
            "0  map<string, int> mp;\\nint main() {\\nint n, sum...  \n",
            "1  const int INF = 0x3f3f3f3f;\\nint main() {\\nint...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2627128550.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  pairs = df.groupby(['probid','subid'], group_keys=False).apply(reconstruct).reset_index()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cpp_to_python_simple(cpp_code):\n",
        "    try:\n",
        "        cpp_code = re.sub(r'#include\\s*<[^>]+>', '', cpp_code)\n",
        "        cpp_code = re.sub(r'using\\s+namespace\\s+std\\s*;', '', cpp_code)\n",
        "        cpp_code = re.sub(r'int\\s+main\\s*\\(\\s*\\)\\s*\\{', 'def main():', cpp_code)\n",
        "        cpp_code = re.sub(r'return\\s+0\\s*;', 'return 0', cpp_code)\n",
        "        cpp_code = re.sub(r'}\\s*$', '', cpp_code)\n",
        "        cpp_code = re.sub(r'cin\\s*>>\\s*([^;]+);', r'\\1 = input()', cpp_code)\n",
        "        cpp_code = re.sub(r'cout\\s*<<\\s*([^;]+);', r'print(\\1)', cpp_code)\n",
        "        cpp_code = re.sub(r'endl', r'', cpp_code)\n",
        "        cpp_code = re.sub(r'//', r'#', cpp_code)\n",
        "        cpp_code = re.sub(r';$', '', cpp_code, flags=re.MULTILINE)\n",
        "        return cpp_code.strip()\n",
        "    except:\n",
        "        return \"print('Hello World')\"\n",
        "\n",
        "print(\"APPLYING C++ TO PYTHON CONVERSION\")\n",
        "pairs['python_code'] = pairs['code'].apply(cpp_to_python_simple)\n",
        "\n",
        "for i in range(2):\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(f\"Pseudocode:\\n{pairs.iloc[i]['pseudo'][:200]}...\")\n",
        "    print(f\"Python:\\n{pairs.iloc[i]['python_code'][:200]}...\")\n",
        "\n",
        "train_df, val_df = train_test_split(pairs, test_size=0.1, random_state=42)\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(val_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4O5GYFqoZ4E",
        "outputId": "1532b3dc-9794-4b89-bc95-1039a761e3f7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "APPLYING C++ TO PYTHON CONVERSION\n",
            "Sample 0:\n",
            "Pseudocode:\n",
            "create a map from strings to integers mp\n",
            "\n",
            "create new integers n and sum with sum = 0\n",
            "create new string variable s\n",
            "read from the input to n\n",
            "for i from 1 to n inclusive, read standard input to s and inc...\n",
            "Python:\n",
            "map<string, int> mp\n",
            "def main():\n",
            "int n, sum = 0\n",
            "string s\n",
            "n = input()\n",
            "for (int i = 1; i <= n; i++) s, mp[s]++ = input()\n",
            "for (int i = 1; i <= n; i++) {\n",
            "s = input()\n",
            "if (mp[s])\n",
            "mp[s]--\n",
            "else\n",
            "sum++\n",
            "}\n",
            "print(s...\n",
            "Sample 1:\n",
            "Pseudocode:\n",
            "INF = const int with INF = 0x3f3f3f3f\n",
            "\n",
            "i, j, k = int\n",
            "n, m = int\n",
            "s, ss = string array of size 105 each\n",
            "read n\n",
            "read n values into s\n",
            "read n values into ss\n",
            "for i = 0 to n\n",
            "for j = 0 to n\n",
            "if s[i] is ss[j] a...\n",
            "Python:\n",
            "const int INF = 0x3f3f3f3f\n",
            "def main():\n",
            "int i, j, k\n",
            "int n, m\n",
            "string s[105], ss[105]\n",
            "n = input()\n",
            "for (i = 0; i < n; i++) s[i] = input()\n",
            "for (i = 0; i < n; i++) ss[i] = input()\n",
            "for (i = 0; i < n; i++) {\n",
            "...\n",
            "Training samples: 13093\n",
            "Validation samples: 1455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "special_tokens = ['<|pseudo|>', '<|python|>', '<|end|>']\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(f\"Special tokens: {special_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8o2ZbnConqJ",
        "outputId": "7932e4de-9be9-46b6-8cd1-3d0800a7a0d0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Special tokens: ['<|pseudo|>', '<|python|>', '<|end|>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_sample_fixed(example):\n",
        "    text = f\"<|pseudo|>{example['pseudo']}<|python|>{example['python_code']}<|end|>\"\n",
        "    encoded = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding='max_length',\n",
        "        return_tensors=None\n",
        "    )\n",
        "    encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n",
        "    return encoded\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df[['pseudo','python_code']]).map(\n",
        "    format_sample_fixed,\n",
        "    remove_columns=['pseudo','python_code']\n",
        ")\n",
        "val_dataset = Dataset.from_pandas(val_df[['pseudo','python_code']]).map(\n",
        "    format_sample_fixed,\n",
        "    remove_columns=['pseudo','python_code']\n",
        ")\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)}\")\n",
        "print(f\"Val dataset: {len(val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "c8c1a69cb0134e95ac39cded57334aaa",
            "378bed31903d4bffa68a15e983935fb4",
            "705f24fbe34d41df94b82a9d9f106bb7",
            "a536b7f737cc43b2bc7592b91e5d2b68",
            "3dc1deafb28e44fa8fdd51f90c4b83d8",
            "6281de1a1a49404f91bf711c7445083d",
            "5b7147900563422d8c9b859a62eb8540",
            "c929061377d844e8ac1c902b5a05bfb2",
            "e1cbd59920434ecb87e72d5ecb31627a",
            "3316c681f2794a10bb35a98c2388e009",
            "c7856e35b2164126adc4473a9d0135f5",
            "8b96ec332be5458cbf226ac7a9f78c33",
            "c4ffae4896d64b91bbf218a44b74a89b",
            "dddbce19507148df89e2f6b1f139e7d8",
            "74c968948f0249cdb3bf4d2cdb717e10",
            "11495b699f2b4136a02ce79b23f61ddd",
            "7168e564e0f64cc98909871c598d2129",
            "538959ff00fd4fbcb0e28565be484733",
            "62490db2816b4251aa7ec8b23164d75e",
            "ef3589af33e34ba4a4b50664d9622cad",
            "4427fd612fa647bba7f8f5245330c6d6",
            "fc5fb69d11564dbc960abac826810170"
          ]
        },
        "id": "dyhQeWJPordD",
        "outputId": "18f5cd98-0d36-42a1-9f76-2618d6f1fb6a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/13093 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8c1a69cb0134e95ac39cded57334aaa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1455 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b96ec332be5458cbf226ac7a9f78c33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: 13093\n",
            "Val dataset: 1455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIEEHQiMowrw",
        "outputId": "d61d7820-7b81-456c-a6a4-923206363327"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 1,179,648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_collator(features):\n",
        "    batch = {\n",
        "        'input_ids': torch.stack([torch.tensor(f['input_ids']) for f in features]),\n",
        "        'attention_mask': torch.stack([torch.tensor(f['attention_mask']) for f in features]),\n",
        "        'labels': torch.stack([torch.tensor(f['labels']) for f in features])\n",
        "    }\n",
        "    return batch\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./pseudo-to-python-model\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=5e-4,\n",
        "    logging_steps=20,\n",
        "    save_steps=800,\n",
        "    eval_steps=800,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    report_to=[],\n",
        "    remove_unused_columns=False,\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "oJyhSXRxo9G6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a06c64-e519-4a68-d0cd-8046573da085"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LoggingTrainer(Trainer):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        super().on_log(args, state, control, logs, **kwargs)\n",
        "        if logs and 'loss' in logs:\n",
        "            print(f\"Step {state.global_step}: Loss = {logs['loss']:.4f}\")\n",
        "        if logs and 'eval_loss' in logs:\n",
        "            print(f\"Step {state.global_step}: Eval Loss = {logs['eval_loss']:.4f}\")\n",
        "\n",
        "trainer = LoggingTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=simple_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"STARTING TRAINING\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"TRAINING COMPLETED\")\n",
        "trainer.save_model(\"./pseudo-to-python-final\")\n",
        "tokenizer.save_pretrained(\"./pseudo-to-python-final\")\n",
        "\n",
        "final_metrics = train_result.metrics\n",
        "print(f\"Final training loss: {final_metrics.get('train_loss', 'N/A')}\")\n",
        "\n",
        "eval_metrics = trainer.evaluate()\n",
        "print(\"FINAL EVALUATION:\")\n",
        "for key, value in eval_metrics.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U4J6Rshjo_92",
        "outputId": "f8b42b10-4555-46f9-f64a-eb7b8be4059f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1341475173.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `LoggingTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = LoggingTrainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING TRAINING\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9822' max='9822' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9822/9822 25:15, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.045500</td>\n",
              "      <td>0.944026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.046200</td>\n",
              "      <td>0.871813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.920500</td>\n",
              "      <td>0.831012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.803600</td>\n",
              "      <td>0.808654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.868500</td>\n",
              "      <td>0.786103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.883800</td>\n",
              "      <td>0.772865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>0.887100</td>\n",
              "      <td>0.761028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>0.763400</td>\n",
              "      <td>0.750383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>0.817700</td>\n",
              "      <td>0.740293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.722500</td>\n",
              "      <td>0.734315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>0.795500</td>\n",
              "      <td>0.728784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9600</td>\n",
              "      <td>0.756800</td>\n",
              "      <td>0.725395</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING COMPLETED\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final training loss: 0.8712590884055462\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='364' max='364' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [364/364 00:18]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL EVALUATION:\n",
            "eval_loss: 0.724769115447998\n",
            "eval_runtime: 19.0023\n",
            "eval_samples_per_second: 76.57\n",
            "eval_steps_per_second: 19.156\n",
            "epoch: 3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate_python_from_pseudo(pseudocode):\n",
        "    input_text = f\"<|pseudo|>{pseudocode}<|python|>\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        num_beams=3,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=2\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "    if \"<|python|>\" in generated_text:\n",
        "        parts = generated_text.split(\"<|python|>\", 1)\n",
        "        if len(parts) > 1:\n",
        "            python_code = parts[1]\n",
        "            if \"<|end|>\" in python_code:\n",
        "                python_code = python_code.split(\"<|end|>\")[0]\n",
        "            return python_code.strip()\n",
        "\n",
        "    return generated_text.replace(input_text, \"\").strip()\n",
        "\n",
        "print(\"TESTING GENERATION:\")\n",
        "test_examples = [\n",
        "    \"print numbers from 1 to 10\",\n",
        "    \"calculate sum of two numbers\",\n",
        "    \"find maximum number in list\"\n",
        "]\n",
        "\n",
        "for i, example in enumerate(test_examples):\n",
        "    generated = generate_python_from_pseudo(example)\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"Input: {example}\")\n",
        "    print(f\"Output: {generated}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDEHmbwbpD1v",
        "outputId": "680784b5-1960-4b10-c034-2265274add32"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TESTING GENERATION:\n",
            "Example 1:\n",
            "Input: print numbers from 1 to 10\n",
            "Output: def main():\n",
            "print(number(1) << )\n",
            "return 0\n",
            "\n",
            "<|endoftext|>\n",
            "\n",
            "Example 2:\n",
            "Input: calculate sum of two numbers\n",
            "Output: def main():\n",
            "int sum(int a, int b) {\n",
            "return a > b ? a : b\n",
            "}\n",
            "long long int calc(double b, double c)\n",
            "print(calc * 2 << \"\\n\")\n",
            "\n",
            "<|endoftext|>\n",
            "\n",
            "Example 3:\n",
            "Input: find maximum number in list\n",
            "Output: def main():\n",
            "list<int> find(maxn)\n",
            "print(find(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) << )\n",
            "return 0\n",
            "<|endoftext|>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "class PseudocodeToPython:\n",
        "    def __init__(self, model_path=\"./pseudo-to-python-final\"):\n",
        "        try:\n",
        "            config = PeftConfig.from_pretrained(model_path)\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            self.tokenizer.add_special_tokens({'additional_special_tokens': ['<|pseudo|>', '<|python|>', '<|end|>']})\n",
        "\n",
        "            base_model = GPT2LMHeadModel.from_pretrained(config.base_model_name_or_path)\n",
        "            base_model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "            self.model = PeftModel.from_pretrained(base_model, model_path)\n",
        "            self.model.eval()\n",
        "\n",
        "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            self.model = self.model.to(self.device)\n",
        "\n",
        "            print(\"Model loaded successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            self.setup_fallback()\n",
        "\n",
        "    def setup_fallback(self):\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.model.eval()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def generate(self, pseudocode):\n",
        "        input_text = f\"<|pseudo|>{pseudocode}<|python|>\"\n",
        "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256,\n",
        "                num_beams=3,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "        if \"<|python|>\" in full_output:\n",
        "            python_code = full_output.split(\"<|python|>\")[1].strip()\n",
        "            if \"<|end|>\" in python_code:\n",
        "                python_code = python_code.split(\"<|end|>\")[0].strip()\n",
        "            return python_code\n",
        "\n",
        "        return full_output.replace(input_text, \"\").strip()\n",
        "\n",
        "def create_interface():\n",
        "    generator = PseudocodeToPython()\n",
        "\n",
        "    def generate_code(pseudocode):\n",
        "        if not pseudocode.strip():\n",
        "            return \"Please enter pseudocode\"\n",
        "        try:\n",
        "            return generator.generate(pseudocode)\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "    with gr.Blocks(title=\"Pseudocode to Python\") as interface:\n",
        "        gr.Markdown(\"# Pseudocode to Python Generator\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                input_text = gr.Textbox(\n",
        "                    label=\"Pseudocode Input\",\n",
        "                    placeholder=\"Enter your pseudocode here...\",\n",
        "                    lines=4\n",
        "                )\n",
        "                generate_btn = gr.Button(\"Generate Python Code\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(\n",
        "                    label=\"Generated Python Code\",\n",
        "                    placeholder=\"Python code will appear here...\",\n",
        "                    lines=4\n",
        "                )\n",
        "\n",
        "        examples = [\n",
        "            \"print numbers from 1 to 10\",\n",
        "            \"calculate sum of two numbers\",\n",
        "            \"find maximum number in list\"\n",
        "        ]\n",
        "\n",
        "        gr.Examples(examples=examples, inputs=input_text)\n",
        "\n",
        "        generate_btn.click(fn=generate_code, inputs=input_text, outputs=output_text)\n",
        "\n",
        "    return interface\n",
        "\n",
        "interface = create_interface()\n",
        "interface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "c7a2Izl_pHOR",
        "outputId": "0dde08f0-9bbd-47bc-b454-5c5922aacdeb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://bfad437f690e99bba8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bfad437f690e99bba8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "folder_path = \"/content/npl task 2\"\n",
        "zip_name = \"gpt2_spoc_lora_FULL2.zip\"\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"GPT-2 LoRA Model Zipper\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# === CHECK IF FOLDER EXISTS ===\n",
        "if not os.path.exists(folder_path):\n",
        "    print(f\"\\n‚ùå ERROR: Folder nahi mila!\")\n",
        "    print(f\"Path: {folder_path}\")\n",
        "    print(\"\\nüîç Current directory files:\")\n",
        "    print(os.listdir('.'))\n",
        "else:\n",
        "    # === LIST ALL FILES ===\n",
        "    files = os.listdir(folder_path)\n",
        "    total_size = 0\n",
        "\n",
        "    print(f\"\\n‚úÖ Folder mil gaya! {len(files)} files hai:\\n\")\n",
        "\n",
        "    for f in files:\n",
        "        file_path = os.path.join(folder_path, f)\n",
        "        size_mb = os.path.getsize(file_path) / (1024 * 1024)  # Convert to MB\n",
        "        total_size += size_mb\n",
        "        print(f\"  üìÑ {f:<40} ({size_mb:.2f} MB)\")\n",
        "\n",
        "    print(f\"\\nüìä Total Size: {total_size:.2f} MB\")\n",
        "\n",
        "    # === CREATE ZIP FILE ===\n",
        "    print(f\"\\n‚è≥ Zip bana raha hoon...\")\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for filename in files:\n",
        "                file_path = os.path.join(folder_path, filename)\n",
        "                zipf.write(file_path, arcname=filename)\n",
        "                print(f\"  ‚úì Added: {filename}\")\n",
        "\n",
        "        # === VERIFY ZIP ===\n",
        "        zip_size = os.path.getsize(zip_name) / (1024 * 1024)\n",
        "        print(f\"\\n‚úÖ ZIP READY!\")\n",
        "        print(f\"üì¶ File: {zip_name}\")\n",
        "        print(f\"üíæ Size: {zip_size:.1f} MB\")\n",
        "\n",
        "        # === DOWNLOAD INSTRUCTIONS ===\n",
        "        print(\"\\n\" + \"=\" * 50)\n",
        "        print(\"üéâ DOWNLOAD KARNE KE LIYE:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # For Google Colab\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            print(\"\\nüîµ Google Colab detected!\")\n",
        "            print(\"Niche button se download hoga...\")\n",
        "            files.download(zip_name)\n",
        "        except ImportError:\n",
        "            # For Jupyter/Local\n",
        "            print(\"\\nüìÅ File ready hai:\")\n",
        "            print(f\"   Location: {os.path.abspath(zip_name)}\")\n",
        "            print(\"\\nüí° Download karne ke liye:\")\n",
        "            print(\"   1. File browser mein dekho (left sidebar)\")\n",
        "            print(\"   2. Right-click ‚Üí Download\")\n",
        "            print(\"\\n   Ya ye command chalaao:\")\n",
        "            print(f'   from IPython.display import FileLink')\n",
        "            print(f'   FileLink(\"{zip_name}\")')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error: {str(e)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "HkeAw9i95WsC",
        "outputId": "109f1661-7699-4f9d-e16b-0cd670ce40be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "GPT-2 LoRA Model Zipper\n",
            "==================================================\n",
            "\n",
            "‚úÖ Folder mil gaya! 9 files hai:\n",
            "\n",
            "  üìÑ special_tokens_map.json                  (0.00 MB)\n",
            "  üìÑ vocab.json                               (0.95 MB)\n",
            "  üìÑ added_tokens.json                        (0.00 MB)\n",
            "  üìÑ adapter_config.json                      (0.00 MB)\n",
            "  üìÑ README.md                                (0.00 MB)\n",
            "  üìÑ adapter_model.safetensors                (297.59 MB)\n",
            "  üìÑ training_args.bin                        (0.01 MB)\n",
            "  üìÑ merges.txt                               (0.44 MB)\n",
            "  üìÑ tokenizer_config.json                    (0.00 MB)\n",
            "\n",
            "üìä Total Size: 298.99 MB\n",
            "\n",
            "‚è≥ Zip bana raha hoon...\n",
            "  ‚úì Added: special_tokens_map.json\n",
            "  ‚úì Added: vocab.json\n",
            "  ‚úì Added: added_tokens.json\n",
            "  ‚úì Added: adapter_config.json\n",
            "  ‚úì Added: README.md\n",
            "  ‚úì Added: adapter_model.safetensors\n",
            "  ‚úì Added: training_args.bin\n",
            "  ‚úì Added: merges.txt\n",
            "  ‚úì Added: tokenizer_config.json\n",
            "\n",
            "‚úÖ ZIP READY!\n",
            "üì¶ File: gpt2_spoc_lora_FULL2.zip\n",
            "üíæ Size: 277.0 MB\n",
            "\n",
            "==================================================\n",
            "üéâ DOWNLOAD KARNE KE LIYE:\n",
            "==================================================\n",
            "\n",
            "üîµ Google Colab detected!\n",
            "Niche button se download hoga...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_be610bf7-6a5e-4324-a059-084d90075e12\", \"gpt2_spoc_lora_FULL2.zip\", 290407767)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VN0VSpvH6_bD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c8c1a69cb0134e95ac39cded57334aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_378bed31903d4bffa68a15e983935fb4",
              "IPY_MODEL_705f24fbe34d41df94b82a9d9f106bb7",
              "IPY_MODEL_a536b7f737cc43b2bc7592b91e5d2b68"
            ],
            "layout": "IPY_MODEL_3dc1deafb28e44fa8fdd51f90c4b83d8"
          }
        },
        "378bed31903d4bffa68a15e983935fb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6281de1a1a49404f91bf711c7445083d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5b7147900563422d8c9b859a62eb8540",
            "value": "Map:‚Äá100%"
          }
        },
        "705f24fbe34d41df94b82a9d9f106bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c929061377d844e8ac1c902b5a05bfb2",
            "max": 13093,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1cbd59920434ecb87e72d5ecb31627a",
            "value": 13093
          }
        },
        "a536b7f737cc43b2bc7592b91e5d2b68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3316c681f2794a10bb35a98c2388e009",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c7856e35b2164126adc4473a9d0135f5",
            "value": "‚Äá13093/13093‚Äá[00:27&lt;00:00,‚Äá505.13‚Äáexamples/s]"
          }
        },
        "3dc1deafb28e44fa8fdd51f90c4b83d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6281de1a1a49404f91bf711c7445083d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b7147900563422d8c9b859a62eb8540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c929061377d844e8ac1c902b5a05bfb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1cbd59920434ecb87e72d5ecb31627a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3316c681f2794a10bb35a98c2388e009": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7856e35b2164126adc4473a9d0135f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b96ec332be5458cbf226ac7a9f78c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4ffae4896d64b91bbf218a44b74a89b",
              "IPY_MODEL_dddbce19507148df89e2f6b1f139e7d8",
              "IPY_MODEL_74c968948f0249cdb3bf4d2cdb717e10"
            ],
            "layout": "IPY_MODEL_11495b699f2b4136a02ce79b23f61ddd"
          }
        },
        "c4ffae4896d64b91bbf218a44b74a89b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7168e564e0f64cc98909871c598d2129",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_538959ff00fd4fbcb0e28565be484733",
            "value": "Map:‚Äá100%"
          }
        },
        "dddbce19507148df89e2f6b1f139e7d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62490db2816b4251aa7ec8b23164d75e",
            "max": 1455,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef3589af33e34ba4a4b50664d9622cad",
            "value": 1455
          }
        },
        "74c968948f0249cdb3bf4d2cdb717e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4427fd612fa647bba7f8f5245330c6d6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fc5fb69d11564dbc960abac826810170",
            "value": "‚Äá1455/1455‚Äá[00:04&lt;00:00,‚Äá445.00‚Äáexamples/s]"
          }
        },
        "11495b699f2b4136a02ce79b23f61ddd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7168e564e0f64cc98909871c598d2129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "538959ff00fd4fbcb0e28565be484733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62490db2816b4251aa7ec8b23164d75e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef3589af33e34ba4a4b50664d9622cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4427fd612fa647bba7f8f5245330c6d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc5fb69d11564dbc960abac826810170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}