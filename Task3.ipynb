{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport logging\nimport torch\nimport pandas as pd\nimport numpy as np\nimport gc\n\nos.system('pip uninstall -y transformers')\nos.system('pip install -q transformers')\nos.system('pip install -q rouge-score sentencepiece')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    T5Tokenizer, T5ForConditionalGeneration,\n    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n    DataCollatorForSeq2Seq\n)\nfrom transformers.utils import logging as hf_logging\nfrom rouge_score import rouge_scorer\n\nhf_logging.set_verbosity_error()\n\ntorch.cuda.empty_cache()\ngc.collect()\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('/kaggle/working/training.log'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\nOUTPUT_DIR = '/kaggle/working'\nCHECKPOINT_DIR = f'{OUTPUT_DIR}/checkpoints'\nFINAL_MODEL_DIR = f'{OUTPUT_DIR}/final_model'\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nos.makedirs(FINAL_MODEL_DIR, exist_ok=True)\n\nMODEL_NAME = 't5-base'\nMAX_INPUT_LENGTH = 256\nMAX_TARGET_LENGTH = 64\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATION = 8\nEPOCHS = 1\nLEARNING_RATE = 5e-4\nWARMUP_STEPS = 100\nSAVE_STEPS = 50\nEVAL_STEPS = 50\nLOGGING_STEPS = 10\n\nlogger.info(\"Loading dataset...\")\ntrain_df = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv')\nval_df = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv')\ntest_df = pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv')\n\ntrain_df = train_df[['article', 'highlights']].dropna()\nval_df = val_df[['article', 'highlights']].dropna()\ntest_df = test_df[['article', 'highlights']].dropna()\n\ntrain_df = train_df.head(5000)\nval_df = val_df.head(500)\ntest_df = test_df.head(300)\n\nlogger.info(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n\ndataset = DatasetDict({\n    'train': Dataset.from_pandas(train_df),\n    'validation': Dataset.from_pandas(val_df),\n    'test': Dataset.from_pandas(test_df)\n})\n\nlogger.info(\"Loading tokenizer and model...\")\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n\ndef preprocess_function(examples):\n    inputs = [\"summarize: \" + doc for doc in examples['article']]\n    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=False)\n    \n    labels = tokenizer(examples['highlights'], max_length=MAX_TARGET_LENGTH, truncation=True, padding=False)\n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs\n\nlogger.info(\"Preprocessing dataset...\")\ntokenized_dataset = dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=dataset['train'].column_names,\n    desc=\"Tokenizing\"\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    \n    if isinstance(predictions, tuple):\n        predictions = predictions[0]\n    \n    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [label.strip() for label in decoded_labels]\n    \n    rouge1_scores = []\n    rouge2_scores = []\n    rougeL_scores = []\n    \n    for pred, label in zip(decoded_preds, decoded_labels):\n        if pred and label:\n            scores = scorer.score(label, pred)\n            rouge1_scores.append(scores['rouge1'].fmeasure)\n            rouge2_scores.append(scores['rouge2'].fmeasure)\n            rougeL_scores.append(scores['rougeL'].fmeasure)\n    \n    return {\n        'rouge1': np.mean(rouge1_scores) if rouge1_scores else 0.0,\n        'rouge2': np.mean(rouge2_scores) if rouge2_scores else 0.0,\n        'rougeL': np.mean(rougeL_scores) if rougeL_scores else 0.0\n    }\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=CHECKPOINT_DIR,\n    eval_strategy='steps',\n    eval_steps=EVAL_STEPS,\n    learning_rate=LEARNING_RATE,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=EPOCHS,\n    predict_with_generate=True,\n    generation_max_length=MAX_TARGET_LENGTH,\n    fp16=True,\n    logging_dir=f'{OUTPUT_DIR}/logs',\n    logging_steps=LOGGING_STEPS,\n    save_steps=SAVE_STEPS,\n    load_best_model_at_end=True,\n    metric_for_best_model='rougeL',\n    greater_is_better=True,\n    warmup_steps=WARMUP_STEPS,\n    report_to='none',\n    dataloader_num_workers=0,\n    dataloader_pin_memory=False,\n    gradient_checkpointing=True,\n    optim='adafactor',\n    max_grad_norm=1.0,\n    logging_first_step=True,\n    generation_num_beams=2\n)\n\nmodel.gradient_checkpointing_enable()\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['validation'],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\nlogger.info(\"Starting training...\")\ntrainer.train()\n\nlogger.info(\"Saving final model...\")\ntrainer.save_model(FINAL_MODEL_DIR)\ntokenizer.save_pretrained(FINAL_MODEL_DIR)\n\nlogger.info(\"Evaluating on test set...\")\ntest_results = trainer.predict(tokenized_dataset['test'])\nlogger.info(f\"Test Results: {test_results.metrics}\")\n\nwith open(f'{OUTPUT_DIR}/test_results.txt', 'w') as f:\n    for key, value in test_results.metrics.items():\n        f.write(f\"{key}: {value}\\n\")\n\nlogger.info(\"Generating example outputs...\")\ntest_samples = test_df.head(10)\nexamples = []\n\nmodel.eval()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor idx, row in test_samples.iterrows():\n    input_text = f\"summarize: {row['article']}\"\n    inputs = tokenizer(input_text, max_length=MAX_INPUT_LENGTH, truncation=True, return_tensors='pt').to(device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            inputs['input_ids'],\n            max_length=MAX_TARGET_LENGTH,\n            num_beams=4,\n            early_stopping=True\n        )\n    \n    generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    examples.append({\n        'article': row['article'][:500] + '...',\n        'original_summary': row['highlights'],\n        'generated_summary': generated_summary\n    })\n\nexamples_df = pd.DataFrame(examples)\nexamples_df.to_csv(f'{OUTPUT_DIR}/example_outputs.csv', index=False)\n\nlogger.info(\"\\n\" + \"=\"*80)\nlogger.info(\"TRAINING SUMMARY\")\nlogger.info(\"=\"*80)\nlogger.info(f\"Model: {MODEL_NAME}\")\nlogger.info(f\"Training samples: {len(train_df)}\")\nlogger.info(f\"Validation samples: {len(val_df)}\")\nlogger.info(f\"Test samples: {len(test_df)}\")\nlogger.info(f\"Epochs: {EPOCHS}\")\nlogger.info(f\"Final Test ROUGE Scores:\")\nlogger.info(f\"  ROUGE-1: {test_results.metrics.get('test_rouge1', 'N/A'):.4f}\")\nlogger.info(f\"  ROUGE-2: {test_results.metrics.get('test_rouge2', 'N/A'):.4f}\")\nlogger.info(f\"  ROUGE-L: {test_results.metrics.get('test_rougeL', 'N/A'):.4f}\")\nlogger.info(\"=\"*80)\n\nlogger.info(\"\\nEXAMPLE OUTPUTS:\")\nfor i, ex in enumerate(examples[:3]):\n    logger.info(f\"\\n{'='*80}\")\n    logger.info(f\"Example {i+1}\")\n    logger.info(f\"{'='*80}\")\n    logger.info(f\"Article (first 300 chars):\\n{ex['article'][:300]}...\")\n    logger.info(f\"\\nOriginal Summary:\\n{ex['original_summary']}\")\n    logger.info(f\"\\nGenerated Summary:\\n{ex['generated_summary']}\")\n    logger.info(f\"{'='*80}\")\n\nlogger.info(f\"\\nAll files saved to: {OUTPUT_DIR}\")\nlogger.info(f\"  - Model: {FINAL_MODEL_DIR}\")\nlogger.info(f\"  - Test results: {OUTPUT_DIR}/test_results.txt\")\nlogger.info(f\"  - Example outputs: {OUTPUT_DIR}/example_outputs.csv\")\nlogger.info(f\"  - Training log: {OUTPUT_DIR}/training.log\")\nlogger.info(\"\\nTraining completed successfully!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T16:05:53.511563Z","iopub.execute_input":"2025-11-01T16:05:53.512052Z","iopub.status.idle":"2025-11-01T17:05:16.771756Z","shell.execute_reply.started":"2025-11-01T16:05:53.512028Z","shell.execute_reply":"2025-11-01T17:05:16.770904Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.57.1\nUninstalling transformers-4.57.1:\n  Successfully uninstalled transformers-4.57.1\n","output_type":"stream"},{"name":"stderr","text":"2025-11-01 16:06:13.934957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762013173.958234    2052 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762013173.965069    2052 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"252516de0e8f42a7958cc087029439a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b48f27572f294bf49a617b2636a7e32b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5db7fa59fb6f4a26ba5f8680a438e2d0"}},"metadata":{}},{"name":"stdout","text":"{'loss': 2.3161, 'grad_norm': 1.9902663230895996, 'learning_rate': 0.0, 'epoch': 0.0128}\n{'loss': 2.3272, 'grad_norm': 1.5101170539855957, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.128}\n{'loss': 2.0791, 'grad_norm': 0.8555189371109009, 'learning_rate': 9.5e-05, 'epoch': 0.256}\n{'loss': 1.9226, 'grad_norm': 0.7838295102119446, 'learning_rate': 0.000145, 'epoch': 0.384}\n{'loss': 1.9347, 'grad_norm': 0.7531446218490601, 'learning_rate': 0.00019500000000000002, 'epoch': 0.512}\n{'loss': 1.9718, 'grad_norm': 0.8020458221435547, 'learning_rate': 0.000245, 'epoch': 0.64}\n{'eval_loss': 1.7801131010055542, 'eval_rouge1': 0.4144453445267999, 'eval_rouge2': 0.19554360219614506, 'eval_rougeL': 0.3045549062365438, 'eval_runtime': 113.1182, 'eval_samples_per_second': 4.42, 'eval_steps_per_second': 0.557, 'epoch': 0.64}\n{'loss': 1.8901, 'grad_norm': 0.8881356716156006, 'learning_rate': 0.000295, 'epoch': 0.768}\n{'loss': 1.9144, 'grad_norm': 0.942176103591919, 'learning_rate': 0.000345, 'epoch': 0.896}\n{'train_runtime': 3415.2215, 'train_samples_per_second': 1.464, 'train_steps_per_second': 0.023, 'train_loss': 1.9929222819171375, 'epoch': 1.0}\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"app_code = \"\"\"import streamlit as st\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport torch\n\nst.set_page_config(page_title=\"T5 Text Summarizer\", page_icon=\"ðŸ“\", layout=\"wide\")\n\n@st.cache_resource\ndef load_model():\n    model_name = \"Ameer15/T5-Text-Summarization\"\n    tokenizer = T5Tokenizer.from_pretrained(model_name)\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    return tokenizer, model\n\nst.title(\"ðŸ“ T5 Text Summarization\")\nst.markdown(\"**Summarize long articles into concise summaries using fine-tuned T5 model**\")\n\ntry:\n    tokenizer, model = load_model()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()\n    \n    col1, col2 = st.columns([2, 1])\n    \n    with col2:\n        st.subheader(\"âš™ï¸ Settings\")\n        max_length = st.slider(\"Summary Length\", 30, 150, 64, help=\"Maximum length of generated summary\")\n        num_beams = st.slider(\"Beam Search\", 2, 8, 4, help=\"Higher values = better quality but slower\")\n        temperature = st.slider(\"Creativity\", 0.5, 2.0, 1.0, 0.1, help=\"Higher = more creative summaries\")\n    \n    with col1:\n        st.subheader(\"ðŸ“„ Input Article\")\n        article = st.text_area(\n            \"Paste your article here:\",\n            height=300,\n            placeholder=\"Enter a long article or news text to summarize...\",\n            help=\"Enter any long-form text content\"\n        )\n        \n        col_btn1, col_btn2, col_btn3 = st.columns([1, 1, 3])\n        with col_btn1:\n            summarize_btn = st.button(\"ðŸš€ Summarize\", type=\"primary\", use_container_width=True)\n        with col_btn2:\n            clear_btn = st.button(\"ðŸ—‘ï¸ Clear\", use_container_width=True)\n    \n    if clear_btn:\n        st.rerun()\n    \n    if summarize_btn:\n        if not article.strip():\n            st.warning(\"âš ï¸ Please enter some text to summarize!\")\n        else:\n            with st.spinner(\"ðŸ”„ Generating summary...\"):\n                input_text = f\"summarize: {article}\"\n                inputs = tokenizer(\n                    input_text,\n                    max_length=512,\n                    truncation=True,\n                    return_tensors='pt'\n                ).to(device)\n                \n                with torch.no_grad():\n                    outputs = model.generate(\n                        inputs['input_ids'],\n                        max_length=max_length,\n                        num_beams=num_beams,\n                        temperature=temperature,\n                        early_stopping=True,\n                        no_repeat_ngram_size=3,\n                        length_penalty=2.0\n                    )\n                \n                summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n                \n                st.subheader(\"âœ¨ Generated Summary\")\n                st.success(summary)\n                \n                col_stats1, col_stats2, col_stats3 = st.columns(3)\n                with col_stats1:\n                    st.metric(\"Original Words\", len(article.split()))\n                with col_stats2:\n                    st.metric(\"Summary Words\", len(summary.split()))\n                with col_stats3:\n                    compression = (1 - len(summary.split()) / len(article.split())) * 100\n                    st.metric(\"Compression\", f\"{compression:.1f}%\")\n    \n    with st.expander(\"â„¹ï¸ About this Model\"):\n        st.markdown(\\\"\\\"\\\"\n        **T5 (Text-to-Text Transfer Transformer)** fine-tuned on CNN/DailyMail dataset.\n        \n        **Model Details:**\n        - Base Model: `t5-base`\n        - Training Samples: 5,000 articles\n        - ROUGE-1: 0.397\n        - ROUGE-2: 0.183\n        - ROUGE-L: 0.285\n        \n        **Best For:**\n        - News articles\n        - Blog posts\n        - Long-form content\n        \n        **Tips:**\n        - Longer texts get better summaries\n        - Adjust beam search for quality/speed tradeoff\n        - Temperature affects creativity\n        \\\"\\\"\\\")\n    \n    with st.expander(\"ðŸ“Š Example Articles\"):\n        st.markdown(\\\"\\\"\\\"\n        **Example 1 - Technology:**\n```\n        Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger. More than squabbles over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans.\n```\n        \n        **Example 2 - Sports:**\n```\n        Dougie Freedman is on the verge of agreeing a new two-year deal to remain at Nottingham Forest. Freedman has stabilised Forest since he replaced cult hero Stuart Pearce and the club's owners are pleased with the job he has done at the City Ground. Nottingham Forest manager Dougie Freedman is set to sign a new contract. Freedman replaced Stuart Pearce as manager in February. The 40-year-old Scot has led Forest to ninth in the Championship table.\n```\n        \\\"\\\"\\\")\n\nexcept Exception as e:\n    st.error(f\"âŒ Error loading model: {e}\")\n    st.info(\"Make sure the model is uploaded to Hugging Face Hub!\")\n\nst.markdown(\"---\")\nst.markdown(\"Made with â¤ï¸ using Streamlit | Model: [Ameer15/T5-Text-Summarization](https://huggingface.co/Ameer15/T5-Text-Summarization)\")\n\"\"\"\n\nwith open('/kaggle/working/app.py', 'w') as f:\n    f.write(app_code)\n\nprint(\"âœ… app.py created successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T17:50:23.779452Z","iopub.execute_input":"2025-11-01T17:50:23.779979Z","iopub.status.idle":"2025-11-01T17:50:23.787631Z","shell.execute_reply.started":"2025-11-01T17:50:23.779958Z","shell.execute_reply":"2025-11-01T17:50:23.786825Z"}},"outputs":[{"name":"stdout","text":"âœ… app.py created successfully!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nimport subprocess\n\nos.chdir('/kaggle/working')\n\n# Create files\ngitignore_content = \"\"\"final_model/\ncheckpoints/\n*.pt\n*.pth\n*.bin\n*.safetensors\n*.log\n__pycache__/\n.ipynb_checkpoints/\n.virtual_documents/\n\"\"\"\n\nrequirements_content = \"\"\"streamlit==1.29.0\ntransformers==4.35.0\ntorch==2.0.1\nsentencepiece==0.1.99\nprotobuf==3.20.3\n\"\"\"\n\nreadme_content = \"\"\"# ðŸ“ T5 Text Summarization\n\nAI-powered text summarization using fine-tuned T5 model on CNN/DailyMail dataset.\n\n## ðŸš€ Features\n- Summarize long articles into concise summaries\n- Adjustable summary length and creativity\n- Based on T5-base architecture\n- ROUGE scores: R-1: 0.397, R-2: 0.183, R-L: 0.285\n\n## ðŸŽ¯ Model\nHosted on Hugging Face: [Ameer15/T5-Text-Summarization](https://huggingface.co/Ameer15/T5-Text-Summarization)\n\n## ðŸ“Š Training Details\n- Dataset: CNN/DailyMail\n- Training Samples: 5,000 articles\n- Validation Samples: 500\n- Test Samples: 300\n- Epochs: 1\n- Base Model: t5-base\n\n## ðŸŒ Live Demo\nTry it on [Streamlit Cloud](https://share.streamlit.io)\n\n## ðŸ’» Local Usage\n```bash\ngit clone https://github.com/Ameer3716/Text_Summarization.git\ncd Text_Summarization\npip install -r requirements.txt\nstreamlit run app.py\n```\n\n## ðŸ“ˆ Performance\n- ROUGE-1: 39.70%\n- ROUGE-2: 18.25%\n- ROUGE-L: 28.49%\n\n## ðŸ› ï¸ Tech Stack\n- Transformers (Hugging Face)\n- PyTorch\n- Streamlit\n- T5 Architecture\n\n## ðŸ“ Example\n**Input:** \"Ever noticed how plane seats appear to be getting smaller...\"\n\n**Output:** \"U.S consumer advisory group says minimum space must be stipulated. Tests conducted by FAA use planes with more leg room than airlines offer.\"\n\n## ðŸ‘¨â€ðŸ’» Author\n**Ameer Sultan**\n- GitHub: [@Ameer3716](https://github.com/Ameer3716)\n- Hugging Face: [@Ameer15](https://huggingface.co/Ameer15)\n\n## ðŸ“„ License\nMIT License\n\"\"\"\n\n# Write files\nwith open('.gitignore', 'w') as f:\n    f.write(gitignore_content)\n\nwith open('requirements.txt', 'w') as f:\n    f.write(requirements_content)\n\nwith open('README.md', 'w') as f:\n    f.write(readme_content)\n\nprint(\"âœ… Files created\")\n\n# Git configuration\nGITHUB_USER = \"Ameer3716\"\nGITHUB_TOKEN = \"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\"\nREPO_NAME = \"Text_Summarization\"\nGIT_EMAIL = \"ameersultan0310@gmail.com\"\nGIT_NAME = \"Ameer\"\n\n# Initialize git\nsubprocess.run(['git', 'init'], check=True)\nsubprocess.run(['git', 'config', 'user.email', GIT_EMAIL], check=True)\nsubprocess.run(['git', 'config', 'user.name', GIT_NAME], check=True)\n\nprint(\"âœ… Git initialized\")\n\n# Add files\nsubprocess.run(['git', 'add', '.gitignore', 'requirements.txt', 'README.md', 'app.py'], check=True)\n\n# Commit\ntry:\n    subprocess.run(['git', 'commit', '-m', 'Add T5 Text Summarization Streamlit app'], check=True)\n    print(\"âœ… Files committed\")\nexcept:\n    print(\"âš ï¸ Nothing to commit or already committed\")\n\n# Add remote\nREMOTE_URL = f\"https://{GITHUB_USER}:{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{REPO_NAME}.git\"\n\ntry:\n    subprocess.run(['git', 'remote', 'add', 'origin', REMOTE_URL], check=True)\nexcept:\n    subprocess.run(['git', 'remote', 'set-url', 'origin', REMOTE_URL], check=True)\n\nprint(\"âœ… Remote configured\")\n\n# Push\nsubprocess.run(['git', 'branch', '-M', 'main'], check=True)\nsubprocess.run(['git', 'push', '-u', 'origin', 'main', '--force'], check=True)\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"âœ… Pushed to https://github.com/{GITHUB_USER}/{REPO_NAME}\")\nprint(\"=\"*60)\nprint(\"\\nNext steps:\")\nprint(\"1. Go to https://share.streamlit.io\")\nprint(\"2. Click 'New app'\")\nprint(f\"3. Repository: {GITHUB_USER}/{REPO_NAME}\")\nprint(\"4. Branch: main\")\nprint(\"5. Main file: app.py\")\nprint(\"6. Deploy!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T17:53:07.301227Z","iopub.execute_input":"2025-11-01T17:53:07.301514Z","iopub.status.idle":"2025-11-01T17:53:08.256201Z","shell.execute_reply.started":"2025-11-01T17:53:07.301496Z","shell.execute_reply":"2025-11-01T17:53:08.255470Z"}},"outputs":[{"name":"stdout","text":"âœ… Files created\nInitialized empty Git repository in /kaggle/working/.git/\nâœ… Git initialized\n[master (root-commit) 5d8fe72] Add T5 Text Summarization Streamlit app\n 4 files changed, 197 insertions(+)\n create mode 100644 .gitignore\n create mode 100644 README.md\n create mode 100644 app.py\n create mode 100644 requirements.txt\nâœ… Files committed\nâœ… Remote configured\n","output_type":"stream"},{"name":"stderr","text":"hint: Using 'master' as the name for the initial branch. This default branch name\nhint: is subject to change. To configure the initial branch name to use in all\nhint: of your new repositories, which will suppress this warning, call:\nhint: \nhint: \tgit config --global init.defaultBranch <name>\nhint: \nhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\nhint: 'development'. The just-created branch can be renamed via this command:\nhint: \nhint: \tgit branch -m <name>\n","output_type":"stream"},{"name":"stdout","text":"Branch 'main' set up to track remote branch 'main' from 'origin'.\n\n============================================================\nâœ… Pushed to https://github.com/Ameer3716/Text_Summarization\n============================================================\n\nNext steps:\n1. Go to https://share.streamlit.io\n2. Click 'New app'\n3. Repository: Ameer3716/Text_Summarization\n4. Branch: main\n5. Main file: app.py\n6. Deploy!\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"To https://github.com/Ameer3716/Text_Summarization.git\n * [new branch]      main -> main\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from huggingface_hub import HfApi, create_repo, login\nimport os\n\nHF_USERNAME = \"Ameer15\"\nHF_TOKEN = \"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\"\nREPO_NAME = f\"{HF_USERNAME}/T5-Text-Summarization\"\n\nprint(\"Logging in to Hugging Face...\")\nlogin(token=HF_TOKEN, add_to_git_credential=True)\n\napi = HfApi()\n\nprint(f\"Creating repository: {REPO_NAME}\")\ncreate_repo(repo_id=REPO_NAME, token=HF_TOKEN, private=False, exist_ok=True, repo_type=\"model\")\nprint(f\"âœ“ Repository ready: https://huggingface.co/{REPO_NAME}\\n\")\n\nmodel_path = \"/kaggle/working/final_model\"\n\nprint(\"Files to upload:\")\nfor root, dirs, files in os.walk(model_path):\n    for file in files:\n        rel_path = os.path.relpath(os.path.join(root, file), model_path)\n        size = os.path.getsize(os.path.join(root, file))\n        print(f\"  - {rel_path} ({size / (1024*1024):.2f} MB)\")\n\nprint(\"\\nUploading to Hugging Face Hub...\")\n\napi.upload_folder(\n    folder_path=model_path,\n    repo_id=REPO_NAME,\n    token=HF_TOKEN,\n    ignore_patterns=[\"*.log\", \"checkpoint-*/*\", \"training_args.bin\"]\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"âœ… Model uploaded successfully!\")\nprint(\"=\"*60)\nprint(f\"ðŸ”— View at: https://huggingface.co/{REPO_NAME}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T17:53:57.261492Z","iopub.execute_input":"2025-11-01T17:53:57.261995Z","iopub.status.idle":"2025-11-01T17:54:09.872629Z","shell.execute_reply.started":"2025-11-01T17:53:57.261975Z","shell.execute_reply":"2025-11-01T17:54:09.871850Z"}},"outputs":[{"name":"stderr","text":"Token has not been saved to git credential helper.\n","output_type":"stream"},{"name":"stdout","text":"Logging in to Hugging Face...\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nCreating repository: Ameer15/T5-Text-Summarization\nâœ“ Repository ready: https://huggingface.co/Ameer15/T5-Text-Summarization\n\nFiles to upload:\n  - special_tokens_map.json (0.00 MB)\n  - spiece.model (0.75 MB)\n  - generation_config.json (0.00 MB)\n  - tokenizer_config.json (0.02 MB)\n  - model.safetensors (850.34 MB)\n  - training_args.bin (0.01 MB)\n  - added_tokens.json (0.00 MB)\n  - config.json (0.00 MB)\n\nUploading to Hugging Face Hub...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b2d25d6b73f4e9cb1bef0acbf01af0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb81364694624bbcb3f06552e9082bd7"}},"metadata":{}},{"name":"stdout","text":"\n============================================================\nâœ… Model uploaded successfully!\n============================================================\nðŸ”— View at: https://huggingface.co/Ameer15/T5-Text-Summarization\n============================================================\n","output_type":"stream"}],"execution_count":12}]}